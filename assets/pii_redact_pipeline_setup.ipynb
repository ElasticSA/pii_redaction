{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN0rxjXtb3OXKF4SdWFid7b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffvestal/pii_redaction/blob/main/assets/pii_redact_pipeline_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automating pii redaction pipeline setup\n",
        "Code to install components for pii pipeline. Details in [this repo](https://github.com/jeffvestal/pii_redaction)"
      ],
      "metadata": {
        "id": "r2LQ23ia4cdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "# Setup\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "cDOAHSLE46JA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and import required packages\n",
        "Elastic uses the [eland python library](https://github.com/elastic/eland) to download modesl from Hugging Face hub and load them into elasticsearch"
      ],
      "metadata": {
        "id": "a6LarBkS5qpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install eland elasticsearch transformers sentence_transformers torch==1.11\n"
      ],
      "metadata": {
        "id": "z_pG1hsh5CKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from eland.ml.pytorch import PyTorchModel\n",
        "from eland.ml.pytorch.transformers import TransformerModel\n",
        "from elasticsearch import Elasticsearch\n",
        "from elasticsearch.client import MlClient\n",
        "from elasticsearch.exceptions import NotFoundError\n",
        "\n",
        "import getpass\n",
        "import requests\n",
        "import json"
      ],
      "metadata": {
        "id": "q88WJrkd5bxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure Elastic Cloud Authentication and Connect\n",
        "The recommended authentication approach is using the [Elastic Cloud ID](https://www.elastic.co/guide/en/cloud/current/ec-cloud-id.html) and a [cluster level API key](https://www.elastic.co/guide/en/kibana/current/api-keys.html)\n",
        "\n",
        "You can use any method you wish to set the required credentials. We are using getpass in this example to prompt for credentials to avoide storing them in github."
      ],
      "metadata": {
        "id": "uxY0lKKk6DtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "es_cloud_id = getpass.getpass('Enter Elastic Cloud ID:  ')\n",
        "es_api_id = getpass.getpass('Enter cluster API key ID:  ') \n",
        "es_api_key = getpass.getpass('Enter cluster API key:  ')"
      ],
      "metadata": {
        "id": "Xw6zGPfl6ZHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = Elasticsearch(cloud_id=es_cloud_id, \n",
        "                   api_key=(es_api_id, es_api_key)\n",
        "                   )\n",
        "es.info() # should return cluster info"
      ],
      "metadata": {
        "id": "oIUu6zCP6YNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "# Loading and Starting the model\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "wNwp_Y5i4rWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the model into Elastic\n",
        "The model we used in testing is the [dslim/bert-base-NER](https://huggingface.co/dslim/bert-base-NER).\n",
        "\n",
        "Any [Elastic compatible NER](https://www.elastic.co/guide/en/machine-learning/current/ml-nlp-model-ref.html#ml-nlp-model-ref-ner) model can be used.\n",
        "\n",
        "The model is downloaded from Hugging Face and then loaded into Elasticsearch for use in the inference processor."
      ],
      "metadata": {
        "id": "C322E-o87V6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_model_id='dslim/bert-base-NER'\n",
        "tm = TransformerModel(hf_model_id, \"ner\")\n",
        "\n",
        "es_model_id = tm.elasticsearch_model_id()\n",
        "es_model_id\n",
        "\n",
        "try:\n",
        "  m = MlClient.get_trained_models(es, model_id=es_model_id)\n",
        "  print ('Model Already Loaded -- Skipping Import')\n",
        "\n",
        "except NotFoundError:\n",
        "  print ('Model NOT currently loaded, loading into Elastic Cluster....')\n",
        "\n",
        "  tmp_path = \"models\"\n",
        "  Path(tmp_path).mkdir(parents=True, exist_ok=True)\n",
        "  model_path, config, vocab_path = tm.save(tmp_path)\n",
        "\n",
        "  ptm = PyTorchModel(es, es_model_id)\n",
        "  ptm.import_model(model_path=model_path, config_path=None, vocab_path=vocab_path, config=config) \n",
        "\n",
        "  #todo add confirmation loading successful"
      ],
      "metadata": {
        "id": "_tDfcNLW4vzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Starting the model for Use"
      ],
      "metadata": {
        "id": "DbwIhwoe8JHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional if you want to see model information in Elastic before starting uncomment and run\n",
        "#m = MlClient.get_trained_models(es, model_id=es_model_id)\n",
        "#m.body"
      ],
      "metadata": {
        "id": "a_3f134v8cyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = MlClient.start_trained_model_deployment(es, model_id=es_model_id)\n",
        "s.body\n",
        "\n",
        "stats = MlClient.get_trained_models_stats(es, model_id=es_model_id)\n",
        "stats.body['trained_model_stats'][0]['deployment_stats']['nodes'][0]['routing_state']"
      ],
      "metadata": {
        "id": "_Gowk4rk83Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "# Loading Ingest Pipeline Config\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "JMSC9egeCbnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the json pipeline definition and confirm it was loaded"
      ],
      "metadata": {
        "id": "c9YgsaG9FWXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/jeffvestal/pii_redaction/main/configuration/ingest_pipeline.pii_redact.json\"\n",
        "\n",
        "response = requests.get(url)\n",
        "pipeline_definition = json.loads(response.text)\n",
        "\n",
        "\n",
        "pipeline_id = 'pii_redaction_pipeline'\n",
        "\n",
        "if es.ingest.put_pipeline(id=pipeline_id, body=pipeline_definition):\n",
        "    print(\"Pipeline created successfully\")\n",
        "else:\n",
        "    print(\"Failed to create pipeline\")\n",
        "\n",
        "\n",
        "pipeline = es.ingest.get_pipeline(id=pipeline_id)\n",
        "pipeline.body"
      ],
      "metadata": {
        "id": "OColXtEZCy9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "# Post Install Configuration\n",
        "---\n",
        "---\n",
        "\n",
        "After the model has been started and the ingest pipeline has been loaded, follow the configuration steps in the [configuration section of read.me](https://)"
      ],
      "metadata": {
        "id": "CV-SBfTDIW-X"
      }
    }
  ]
}